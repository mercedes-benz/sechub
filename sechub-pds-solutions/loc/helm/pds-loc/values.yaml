# SPDX-License-Identifier: MIT

# This is a sample values file containing the defaults.

# Number of SecHub server instances to spin up
replicaCount: 1

image:
  registry: "ghcr.io/mercedes-benz/sechub/pds-loc"
  tag: "latest"
  # Optional: If your image is in a private registry, you can specify pull secrets defined in k8s
  #   Docs: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  #   Example:
  # imagePullSecrets: |-
  #   - name: sechub-pull-secret
  #   - name: sps2

resources:
  requests:
    # Initial container memory size
    memory: 256Mi
  limits:
    # Maximum container memory size
    memory: 2Gi

pds:
  # Maximum upload size: 100 MiB by default (100 * 1024 * 1024 = 104857600)
  uploadMaximumBytes: "104857600"
  config:
    execute:
      # Maximum queue size of pds
      queueMax: 10
      # Number of parallel threads for processing
      workerThreadCount: 5
  volumes:
    workspace:
      # Size of /workspace where the uploads are unpacked
      size: 1Gi
  # Print alive status every minute
  heartbeatLogging: "true"
  logging:
    type:
      # loggingType (optional): See https://mercedes-benz.github.io/sechub/latest/sechub-operations.html#logging-2
      # https://github.com/mercedes-benz/sechub/blob/develop/sechub-server/src/main/resources/logback-spring.xml
      enabled: false
      appenderName: "LOGSTASH_JSON"
  debug:
    # Automatically cleans workspace. Disable for debugging if you need to look at the files
    keepReportsInWorkspace: false
  javaDebug:
    # Enable Java debugging
    enabled: false
  # When set to true, then the container's run.sh script will keep running for 2h on SecHub server exit.
  # Useful for analyzing server crashes. In all other cases: set to `false`.
  keepContainerAliveAfterPDSCrashed: false

users:
  # PDS user for scanning
  technical:
    id: "techuser"
    apiToken: "<api token of techuser>"
  # PDS user with admin permissions
  admin:
    id: "admin"
    apiToken: "<api token of admin user>"

storage:
  local:
    # No shared storage - only for standalone SecHub server (stores into local /tmp)
    enabled: true
  s3:
    # Use S3 as shared storage backend
    enabled: false
    endpoint: "https://<minio-or-s3-server>:<port>"
    bucketname: "<my-bucketname>"
    accesskey: "<my-accesskey>"
    secretkey: "<my-secretkey>"
  sharedVolume:
    # Use a shared filesystem as storage backend
    enabled: false
    upload:
      dir: "/mount/nfs/shares/<sharename>"

database:
  postgres:
    enabled: false
    connection: "jdbc:postgresql://database:5432/pds"
    username: "pds_loc"
    password: "<password-of-db-user>"

networkPolicy:
  # Enables network access from the SecHub server
  enabled: false
  ingress:
    - from:
      - podSelector:
          matchLabels:
            name: sechub-server
      - podSelector:
          matchLabels:
            name: sechub-adminserver

# deploymentComment (optional):
#  When setting to a different value than before, it forces k8s to spin up a new container.
#  This way, you can force deployments e.g. when only secrets have changed.
deploymentComment: "my deployment comment"

# optional: Add annotations to template.metadata.annotations
#           Can contain multiple lines. Example for Prometheus actuator:
# templateMetadataAnnotations: |-
#   prometheus.io/scrape: "true"
#   prometheus.io/probe: "true"
#   prometheus.io/port: "10251"
#   prometheus.io/path: "/actuator/prometheus"
