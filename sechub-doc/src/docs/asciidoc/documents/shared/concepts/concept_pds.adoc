// SPDX-License-Identifier: MIT
=== Product delegation server

==== General
There are many open source clients available having no server component inside so lacking:

- REST access
- queuing
- status requests
- scalable
- â€¦ more 

So when we want to adapt them in {sechub} style _(the product does the work and we ask for result)_ we 
need to provide a `ProductDelegationServer` (in short form `PDS`).

`PDS` is

- a spring boot application which uses a network DB for providing cluster possibility
- a complete standalone application without runtime dependencies to sechub server (or its shared kernel)
- provides REST access
- a very simple priviledge model with just two users (`tech user` + `admin user`), 
  basic auth via `TLS`, credentials are simply defined by environment entries on startup
- provides jobs, queing, monitoring etc.
- can execute single files (e.g. a bash script), where job parameters are
  available as environment variables at runtime 
- a standard way to integrate any product into {sechub} in a scalable and easy way 

==== Handling of resources
- PDS server provides `auto unzipping` of uploaded resources when configured  - see <<section-pds-server-config-file,PDS server configuration file>>
- When a PDS job fails or is done the resources inside job workspace location are *automatically removed*

==== Big picture
plantuml::diagrams/diagram_concept_product_delgation_server_bigpicture.puml[]


[[pds-storage-and-sharing]]
==== Storage and sharing

{pds} needs - like {sechub} - the possiblity to store job data in a central storage location when 
operating inside a cluster _(it's not clear which cluster member uploads job data and which one does execute 
the job and need the data at exectuion time)._

{pds} does use the already existing `sechub-storage-*` libraries which provide storage mechanism for S3 and 
for shared volumes.

[TIP]
====
The next examples explain the different situations appearing for a PDS cluster executing product "XYZ", 
but of course this applies to multiple clusters for different products as well
====

[NOTE]
====
For non clustered environment "shared volumes" can also be local file system paths like
`/opt/storage/pds/XYZ`
====

[WARNING]
====
When talking about a NFS or a shared volume here, this is always meant to be a file system path.
It's important to mount a NFS at a local file path for the PDS server when using a shared volume.
URLs are NOT supported - only filepathes. So you must mount your network file storage to a local file location!

For example: a base path for a shared volume could look like: `/media/mounted-storage/`
====


===== Shared S3 storage

In the <<section-pds-s3-storage-example-shared-s3-buckets,next example>> PDS and SecHub are using the same S3 bucket to store files uploaded by the user.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {secHub} job and it is stored at shared S3 bucket _(blue lines)_.
  - When {secHub} job is starting it will create a {pds} job but does not upload any additional data. 
  - When PDS job is starting the PDS server will fetch workspace files from existing {secHub} job data _(red lines)_. 
  - The storage path location is for {pds} and {secHub}  `${sharedS3Bucket}/jobstorage/${secHubJobUUID}/`
  

[[section-pds-s3-storage-example-shared-s3-buckets]]
plantuml::diagrams/diagram_concept_pds_storage_example_shared_s3_bucket.puml[]

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is NOT like in picture before, but more like in <<pds-storage-different-s3,Different S3 storages>>.
  Read there for more details - it's the same - except there are not 2 different S3 buckets but only one.
  
  
[[pds-storage-different-s3]]
===== Different S3 storages
In the <<section-pds-s3-storage-example-diff-s3-buckets,next example>> PDS and SecHub are using different S3 buckets as storage.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {secHub} job and it is stored at shared S3 bucket _(blue lines)_.
  - The storage path location for {secHub} is `${sechubS3Bucket}/jobstorage/${secHubJobUUID}/` 
  - When {secHub} job is starting it will create and initialize a {pds} job by uploading all existing job data by {pds} rest API. 
    This will store job data at {pds} storage. _(green lines)_.
  - When PDS job is starting, the  PDS server will fetch workspace files from its {pds} job data _(red lines)_. 
  - The storage path location for {pds} is `${pdsS3Bucket}/pds/${pdsProductIdentifier}/${pdsJobUUID}/` 


[[section-pds-s3-storage-example-diff-s3-buckets]]
plantuml::diagrams/diagram_concept_pds_storage_example_different_s3_buckets.puml[]


* Product executor configuration `pds.config.use.sechub.storage` is `true`

WARNING: This will NOT WORK! The job storage will not be found and an error will be thrown at job execution time.
 
===== Same shared volume (NFS)
In the <<section-pds-s3-storage-example-shared-nfs,next example>> PDS server and SecHub are using same shared volume as storage.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {secHub} job and it is stored at shared volume _(blue lines)_.
  - When {secHub} job is starting it will create a {pds} job but does not upload any additional data. 
  - When PDS job is starting the PDS server will fetch workspace files from existing {secHub} job data _(red lines)_. 
  - The storage path location is for {pds} and {secHub}  `${sharedVolumeBasePath}/jobstorage/${secHubJobUUID}/`
  
[[section-pds-s3-storage-example-shared-nfs]]
plantuml::diagrams/diagram_concept_pds_storage_example_shared_nfs.puml[]

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is NOT like in picture before, but more like in <<pds-storage-different-sharedvolume,Different shared volumes>>.
  Read there for more details - it's the same - except there are not 2 different NFS but only one.
  
  
[[pds-storage-different-sharedvolume]]
===== Different shared volumes (NFS)
In the <<section-pds-s3-storage-example-diff-nfs,next example>> PDS and SecHub are using different shared volumes as storage.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {secHub} job and it is stored at sechub shared volume _(blue lines)_.
  - The storage path location for {secHub} is `${sechubSharedVolumeBasePath}/jobstorage/${secHubJobUUID}/` 
  - When {secHub} job is starting it will create and initialize a {pds} job by uploading all existing job data by {pds} rest API. 
    This will store job data at {pds} storage. _(green lines)_.
  - When PDS job is starting, the  PDS server will fetch workspace files from its {pds} job data _(red lines)_. 
  - The storage path location for {pds} is `${pdsSharedVolumeBasePath}/pds/${pdsProductIdentifier}/${pdsJobUUID}/` 
 
[[section-pds-s3-storage-example-diff-nfs]]
plantuml::diagrams/diagram_concept_pds_storage_example_different_nfs.puml[]


* Product executor configuration `pds.config.use.sechub.storage` is `true`

WARNING: This will NOT WORK! The job storage will not be found and an error will be thrown at job execution time.
 
[[pds-storage-mixing-sharedvolume-and-s3]]
===== Mixing S3 and shared volume (NFS) 
This <<section-pds-s3-storage-example-s3-nfs-mixed,example>> is only mentioned for the sake of completeness: It is the same as before described for different S3 and different shared volumes: 
`pds.config.use.sechub.storage` cannot be used in this case.

When not reusing {secHub} storage, this scenario does work also well. In the next picture, {secHub} itself would use a S3 storage and he 
PDS instances for product `XYZ` would use a NFS to store job data. But of course it could be also the other way. 

[[section-pds-s3-storage-example-s3-nfs-mixed]]
plantuml::diagrams/diagram_concept_pds_storage_example_s3_and_nfs_mixed.puml[]


==== SecHub integration
===== Executors and Adapters
====== Executors
With `PDS` there is a default `REST` `API` available. 

For different scanTypyes there will be dedicated PDSExecutors 
(`PDSWebScanExecutor`, `PDSInfraScanExecutor`, etc., etc.)

====== Adapters
The Adapter will always be the same, but filled with other necessary parameters.

NOTE: So there will be no need to write any adapter or executor when using PDS! 


=== HowTo integrate a new product via PDS

Having new security product XYZ but being a command line tool, we 


- create an environment (e.g. a docker container) where all your needed parts are integrated. 
  E.g. bash shell, tools, the product and its dependencies etc.

- create an executable starter script (e.g. bash) which
  * calls the product
  * does system out/err to standard pipes    
  * writes the product result report to relative path `./output/result.txt`

- create a `PSD` configuration file and fill with necessary data, see <<section-pds-server-config-file,PDS server configuration file>> 
 
- start wanted amount of `ProductDelegationServer` instances with dedicated configuration 
  setup to have a clustered, server ready execution of CLI security products. If you want your
  PDS to be started inside a cluster you have to setup load balancing etc. by your own. 
  For example: When using Kubernetes you normally would do this by just defining a 
  `Service` pointing to your `PODs`.
  
- test via developer admin UI if the servers are working:
  * create test job, remember PDS job uuid
  * upload data
  * mark job as ready to start
  * check job state
  * fetch result and inspect

- when former test was successful
  * Define executor at sechub server side - *will be implemented with #148*
  * When your product uses sereco general report format your are done- *will be implemented with #283* 
    otherwise SERECO must have a logic to import custom format for the PRODUCT - means needs an
    implementatiion
    
- test via sechub client by start a new sechub job which shall use the product and verify results
    



