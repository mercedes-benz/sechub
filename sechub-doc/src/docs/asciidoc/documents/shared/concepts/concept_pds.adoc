// SPDX-License-Identifier: MIT

// pds general describes the basics
include::concept_pds_general.adoc[]

==== Encryption
include::concept_pds_data_encryption.adoc[]

[[concept-pds-auto-cleanup]]
==== Auto cleanup
The PDS provides an auto cleanup mechanism which will remove old PDS jobs and their data automatically.

The default configuration is set to 2 days.
Administrators can change the default configuration via <<section-rest-autocleanup,REST>> .



[[pds-storage-and-sharing]]
==== Storage and sharing

{pds} needs - like {sechub} - the possiblity to store job data in a central storage location when
operating inside a cluster _(it's not clear which cluster member uploads job data and which one does execute
the job and need the data at exectuion time)._

{pds} does use the already existing `sechub-storage-*` libraries which provide storage mechanism for S3 and
for shared volumes.

[TIP]
====
The next examples explain the different situations appearing for a PDS cluster executing product "XYZ",
but of course this applies to multiple clusters for different products as well
====

[NOTE]
====
For non clustered environment "shared volumes" can also be local file system paths like
`/opt/storage/pds/XYZ`
====

[WARNING]
====
When talking about a NFS or a shared volume here, this is always meant to be a file system path.
It's important to mount a NFS at a local file path for the PDS server when using a shared volume.
URLs are NOT supported - only file paths. So you must mount your network file storage to a local file location!

For example: a base path for a shared volume could look like: `/media/mounted-storage/`
====


===== Shared S3 storage

In the <<section-pds-s3-storage-example-shared-s3-buckets,next example>> PDS and SecHub are using the same S3 bucket to store files uploaded by the user.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {sechub} job and it is stored at shared S3 bucket _(blue lines)_.
  - When {sechub} job is starting it will create a {pds} job but does not upload any additional data.
  - When PDS job is starting the PDS server will fetch workspace files from existing {sechub} job data _(red lines)_.
  - The storage path location is for {pds} and {sechub}  `${sharedS3Bucket}/jobstorage/${secHubJobUUID}/`


[[section-pds-s3-storage-example-shared-s3-buckets]]
plantuml::diagrams/diagram_concept_pds_storage_example_shared_s3_bucket.puml[]

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is NOT like in picture before, but more like in <<pds-storage-different-s3,Different S3 storages>>.
  Read there for more details - it's the same - except there are not 2 different S3 buckets but only one.


[[pds-storage-different-s3]]
===== Different S3 storages
In the <<section-pds-s3-storage-example-diff-s3-buckets,next example>> PDS and SecHub are using different S3 buckets as storage.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {sechub} job and it is stored at shared S3 bucket _(blue lines)_.
  - The storage path location for {sechub} is `${sechubS3Bucket}/jobstorage/${secHubJobUUID}/`
  - When {sechub} job is starting it will create and initialize a {pds} job by uploading all existing job data by {pds} rest API.
    This will store job data at {pds} storage. _(green lines)_.
  - When PDS job is starting, the  PDS server will fetch workspace files from its {pds} job data _(red lines)_.
  - The storage path location for {pds} is `${pdsS3Bucket}/pds/${pdsProductIdentifier}/${pdsJobUUID}/`


[[section-pds-s3-storage-example-diff-s3-buckets]]
plantuml::diagrams/diagram_concept_pds_storage_example_different_s3_buckets.puml[]


* Product executor configuration `pds.config.use.sechub.storage` is `true`

WARNING: This will NOT WORK! The job storage will not be found and an error will be thrown at job execution time.

===== Same shared volume (NFS)
In the <<section-pds-s3-storage-example-shared-nfs,next example>> PDS server and SecHub are using same shared volume as storage.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {sechub} job and it is stored at shared volume _(blue lines)_.
  - When {sechub} job is starting it will create a {pds} job but does not upload any additional data.
  - When PDS job is starting the PDS server will fetch workspace files from existing {sechub} job data _(red lines)_.
  - The storage path location is for {pds} and {sechub}  `${sharedVolumeBasePath}/jobstorage/${secHubJobUUID}/`

[[section-pds-s3-storage-example-shared-nfs]]
plantuml::diagrams/diagram_concept_pds_storage_example_shared_nfs.puml[]

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is NOT like in picture before, but more like in <<pds-storage-different-sharedvolume,Different shared volumes>>.
  Read there for more details - it's the same - except there are not 2 different NFS but only one.


[[pds-storage-different-sharedvolume]]
===== Different shared volumes (NFS)
In the <<section-pds-s3-storage-example-diff-nfs,next example>> PDS and SecHub are using different shared volumes as storage.

We have two different scenarios here:

* Product executor configuration `pds.config.use.sechub.storage` is NOT `true` +
  In this case the way is exactly as shown in next picture: +
  - The user uploads job data for a {sechub} job and it is stored at {sechub} shared volume _(blue lines)_.
  - The storage path location for {sechub} is `${sechubSharedVolumeBasePath}/jobstorage/${secHubJobUUID}/`
  - When {sechub} job is starting it will create and initialize a {pds} job by uploading all existing job data by {pds} rest API.
    This will store job data at {pds} storage. _(green lines)_.
  - When PDS job is starting, the  PDS server will fetch workspace files from its {pds} job data _(red lines)_.
  - The storage path location for {pds} is `${pdsSharedVolumeBasePath}/pds/${pdsProductIdentifier}/${pdsJobUUID}/`

[[section-pds-s3-storage-example-diff-nfs]]
plantuml::diagrams/diagram_concept_pds_storage_example_different_nfs.puml[]


* Product executor configuration `pds.config.use.sechub.storage` is `true`

WARNING: This will NOT WORK! The job storage will not be found and an error will be thrown at job execution time.

[[pds-storage-mixing-sharedvolume-and-s3]]
===== Mixing S3 and shared volume (NFS)
This <<section-pds-s3-storage-example-s3-nfs-mixed,example>> is only mentioned for the sake of completeness: It is the same as before described for different S3 and different shared volumes:
`pds.config.use.sechub.storage` cannot be used in this case.

When not reusing {sechub} storage, this scenario does work also well. In the next picture, {sechub} itself would use a S3 storage and he
PDS instances for product `XYZ` would use a NFS to store job data. But of course it could be also the other way.

[[section-pds-s3-storage-example-s3-nfs-mixed]]
plantuml::diagrams/diagram_concept_pds_storage_example_s3_and_nfs_mixed.puml[]

[[pds-process-handling]]
==== Process execution
PDS instances are executing so called `caller scripts` by spanning a new process. At this time
dedicated environment variables are automatically injected and available inside the scripts.

[TIP]
====
When implementing a new PDS product integration you should always start with a normal,
executable script (e.g. `bash`).

So you can simply call your script standalone and when it works you just have to create
the PDS configuration file and make the script callable.
====

===== How PDS provides output and error stream content of running jobs in clustered environments
include::concept_pds_fetch_stream_of_running_job.adoc[]

===== How PDS handles meta data
include::concept_pds_handle_metadata.adoc[]

===== How PDS handles storage data
include::concept_pds_handle_storage_parts_at_runtime.adoc[]

===== How PDS handles user messages
include::concept_product_message_pds_dataflow.adoc[]

===== How PDS handles execution events
include::concept_pds_events_dataflow.adoc[]

===== How PDS product executors configure the PDS adapter
include::concept_pds_adapter_configuration.adoc[]

===== How PDS handles deployment with running jobs
include::concept_pds_deployment_with_running_jobs.adoc[]

==== SecHub integration
===== Executors and Adapters
====== Executors
With `PDS` there is a default `REST` `API` available.

For different scanTypyes there will be dedicated PDSExecutors
(`PDSWebScanExecutor`, `PDSInfraScanExecutor`, etc., etc.)

====== Adapters
The Adapter will always be the same, but filled with other necessary parameters.

NOTE: So there will be no need to write any adapter or executor when using PDS!


==== HowTo integrate a new product via PDS

Having new security product XYZ but being a command line tool, we


- create an environment (e.g. a docker container) where all your needed parts are integrated.
  E.g. bash shell, tools, the product and its dependencies etc.

- create an executable starter script (e.g. bash) which
  * calls the product
  * does system out/err to standard pipes
  * writes the product result report to relative path `./output/result.txt`

- create a `PSD` configuration file and fill with necessary data, see <<section-pds-server-config-file,PDS server configuration file>>

- start wanted amount of `ProductDelegationServer` instances with dedicated configuration
  setup to have a clustered, server ready execution of CLI security products. If you want your
  PDS to be started inside a cluster you have to setup load balancing etc. by your own.
  For example: When using Kubernetes you normally would do this by just defining a
  `Service` pointing to your `PODs`.

- test via developer admin UI if the servers are working:
  * create test job, remember PDS job uuid
  * upload data
  * mark job as ready to start
  * check job state
  * fetch result and inspect

- when former test was successful
  * Define executor on {sechub} server side - *will be implemented with #148*
  * When your product uses sereco general report format your are done- *will be implemented with #283*
    otherwise SERECO must have a logic to import custom format for the PRODUCT - means needs an
    implementatiion

- test via {sechub} client by starting a new {sechub} job which shall use the product and verify results

[CAUTION]
====
Output and error stream of a PDS launcher script are stored in {pds} database as plain text!
Means: NEVER log any sensitive data in launcher scripts! 

If you want to give hints for debugging etc. you have to mask the information in log output. 
====



